{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br> Final Proejct: Text to Image Synthesis (Tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) Data Science & AI Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For understanding of this work, please carefully look at given PPT file.**\n",
    "\n",
    "**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the training process **</font> so that TAs can grade both your code and results.  \n",
    "**The TA will set a config file as 'eval_birds.yml' when evaluating the code using 'hidden test dataset'. Thus, please make sure that your code can generate proper data to measure inception score and R-precision of 'hidden test dataset'.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load datasets\n",
    "The Birds dataset will be downloaded automatically if it is not located in the *data* directory. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os, nltk\n",
    "\n",
    "from miscc.config import cfg, cfg_from_file\n",
    "import pprint\n",
    "import datetime\n",
    "import dateutil.tz\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "from utils.data_utils import CUBDataset\n",
    "from utils.loss import cosine_similarity\n",
    "\n",
    "#################################################\n",
    "# DO NOT CHANGE \n",
    "from utils.model import CNN_ENCODER, RNN_ENCODER, GENERATOR, DISCRIMINATOR\n",
    "#################################################\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a config file as 'train_birds.yml' in training, as 'eval_birds.yml' for evaluation\n",
    "cfg_from_file('cfg/train_birds.yml') # eval_birds.yml\n",
    "\n",
    "print('Using config:')\n",
    "pprint.pprint(cfg)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = cfg.GPU_ID\n",
    "\n",
    "now = datetime.datetime.now(dateutil.tz.tzlocal())\n",
    "timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
    "output_dir = 'sample/%s_%s_%s' % (cfg.DATASET_NAME, cfg.CONFIG_NAME, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = CUBDataset(cfg.DATA_DIR, split='train')\n",
    "test_dataset = CUBDataset(cfg.DATA_DIR, split='test')\n",
    "\n",
    "print(f'\\ntrain data directory:\\n{train_dataset.split_dir}')\n",
    "print(f'test data directory:\\n{test_dataset.split_dir}\\n')\n",
    "\n",
    "print(f'# of train filenames:{train_dataset.filenames.shape}')\n",
    "print(f'# of test filenames:{test_dataset.filenames.shape}\\n')\n",
    "\n",
    "print(f'example of filename of train image:{train_dataset.filenames[0]}')\n",
    "print(f'example of filename of valid image:{test_dataset.filenames[0]}\\n')\n",
    "\n",
    "print(f'example of caption and its ids:\\n{train_dataset.captions[0]}\\n{train_dataset.captions_ids[0]}\\n')\n",
    "print(f'example of caption and its ids:\\n{test_dataset.captions[0]}\\n{test_dataset.captions_ids[0]}\\n')\n",
    "\n",
    "print(f'# of train captions:{np.asarray(train_dataset.captions).shape}')\n",
    "print(f'# of test captions:{np.asarray(test_dataset.captions).shape}\\n')\n",
    "\n",
    "print(f'# of train caption ids:{np.asarray(train_dataset.captions_ids).shape}')\n",
    "print(f'# of test caption ids:{np.asarray(test_dataset.captions_ids).shape}\\n')\n",
    "\n",
    "print(f'# of train images:{train_dataset.images.shape}')\n",
    "print(f'# of test images:{test_dataset.images.shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define models and go to train/evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###======================== DEFIINE VARIABLES ===================================###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###======================== DEFIINE PLACEHOLDER ===================================###\n",
    "t_real_image = tf.placeholder('float32', [cfg.BATCH_SIZE, cfg.IMAGE_SIZE, cfg.IMAGE_SIZE, 3], name = 'real_image')\n",
    "t_real_caption = tf.placeholder(dtype=tf.int32, shape=[cfg.BATCH_SIZE , None], name='real_caption_input')\n",
    "t_wrong_image = tf.placeholder('float32', [cfg.BATCH_SIZE ,cfg.IMAGE_SIZE, cfg.IMAGE_SIZE, 3], name = 'wrong_image')\n",
    "t_wrong_caption = tf.placeholder(dtype=tf.int32, shape=[cfg.BATCH_SIZE , None], name='wrong_caption_input')\n",
    "t_z = tf.placeholder(tf.float32, [cfg.BATCH_SIZE , cfg.GAN.Z_DIM], name='z_noise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_encoder = RNN_ENCODER(t_real_caption, is_training=False, reuse=False)\n",
    "generator = GENERATOR(t_z, rnn_encoder.outputs, is_training=False, reuse=False)\n",
    "discriminator = DISCRIMINATOR(generator.outputs, is_training=False, reuse=False)\n",
    "cnn_encoder = CNN_ENCODER(t_real_image, is_training=False, reuse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define image and text mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define optimzers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train\n",
    "sess = tf.Session(config=tf.ConfigProto())\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "saver = tf.train.Saver(var_list=tf.global_variables(), max_to_keep=100)\n",
    "for epoch in range(cfg.TRAIN.MAX_EPOCH):\n",
    "    #################################################\n",
    "    # TODO: Implement text to image synthesis\n",
    "\n",
    "\n",
    "    #################################################\n",
    "    # save checkpoints\n",
    "    checkpoint_path = os.path.join(cfg.CHECKPOINT_DIR, cfg.CHECKPOINT_NAME)\n",
    "    saver.save(sess, checkpoint_path, global_step=epoch)\n",
    "    print('The checkpoint has been created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_r_precision_data():\n",
    "    caption_ids = np.reshape(np.asarray(test_dataset.captions_ids), (-1, cfg.TEXT.WORDS_NUM))\n",
    "    captions_ids_wrong = np.reshape(test_dataset.random_wrong_captions(), (-1, cfg.WRONG_CAPTION, cfg.TEXT.WORDS_NUM))\n",
    "\n",
    "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # load the trained checkpoint\n",
    "    checkpoint_dir = cfg.CHECKPOINT_DIR\n",
    "    if checkpoint_dir is not None:\n",
    "        loader = tf.train.Saver(var_list=tf.global_variables())\n",
    "        ckpt_path = os.path.join(cfg.CHECKPOINT_DIR, CHECKPOINT_NAME)\n",
    "        loader.restore(sess, ckpt_path)\n",
    "        print(\"Restored model parameters from {}\".format(ckpt_path))\n",
    "    else:\n",
    "        print('no checkpoints find.')\n",
    "\n",
    "    n_caption_test = len(caption_ids)\n",
    "    num_batches = n_caption_test // cfg.BATCH_SIZE\n",
    "\n",
    "    true_cnn_features = np.zeros((num_batches, cfg.BATCH_SIZE, cfg.TEXT.EMBEDDING_DIM), dtype=float)\n",
    "    true_rnn_features = np.zeros((num_batches, cfg.BATCH_SIZE, cfg.TEXT.EMBEDDING_DIM), dtype=float)\n",
    "    wrong_rnn_features = np.zeros((num_batches, cfg.WRONG_CAPTION, cfg.BATCH_SIZE, cfg.TEXT.EMBEDDING_DIM), dtype=float)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        test_cap = caption_ids[i * cfg.BATCH_SIZE: (i + 1) * cfg.BATCH_SIZE]\n",
    "\n",
    "        z = np.random.normal(loc=0.0, scale=1.0, size=(cfg.BATCH_SIZE, cfg.GAN.Z_DIM)).astype(np.float32)\n",
    "        \n",
    "        rnn_features = sess.run(rnn_encoder.outputs, feed_dict={t_real_caption: test_cap})\n",
    "        gen = sess.run(generator.outputs, feed_dict={t_real_caption: test_cap, t_z: z})\n",
    "        cnn_features = sess.run(cnn_encoder.outputs, feed_dict={t_real_image: gen})\n",
    "\n",
    "        true_cnn_features[i] = cnn_features\n",
    "        true_rnn_features[i] = rnn_features\n",
    "\n",
    "        for per_wrong_caption in range(cfg.WRONG_CAPTION):\n",
    "            test_cap = captions_ids_wrong[i * cfg.BATCH_SIZE: (i + 1) * cfg.BATCH_SIZE]\n",
    "            rnn_features = sess.run(rnn_encoder.outputs, feed_dict={t_real_caption: test_cap[:, per_wrong_caption]})\n",
    "            wrong_rnn_features[i, per_wrong_caption] = rnn_features\n",
    "    \n",
    "    # if exists, remove the existing file first\n",
    "    try:\n",
    "        os.remove(os.path.join(cfg.R_PRECISION_DIR, cfg.R_PRECISION_FILE))\n",
    "    except OSError:\n",
    "        pass\n",
    "    np.savez(os.path.join(cfg.R_PRECISION_DIR, cfg.R_PRECISION_FILE), true_cnn=true_cnn_features, true_rnn=true_rnn_features,\n",
    "             wrong_rnn=wrong_rnn_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inception_score_data():\n",
    "    caption_ids = np.reshape(np.asarray(test_dataset.captions_ids),\n",
    "                             (-1, cfg.TEXT.CAPTIONS_PER_IMAGE, cfg.TEXT.WORDS_NUM))\n",
    "    \n",
    "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    checkpoint_dir = cfg.CHECKPOINT_DIR\n",
    "    if checkpoint_dir is not None:\n",
    "        loader = tf.train.Saver(var_list=tf.global_variables())\n",
    "        ckpt_path = os.path.join(cfg.CHECKPOINT_DIR, cfg.CHECKPOINT_NAME)\n",
    "        loader.restore(sess, ckpt_path)\n",
    "        print(\"Restored model parameters from {}\".format(ckpt_path))\n",
    "    else:\n",
    "        print('no checkpoints find.')\n",
    "\n",
    "    n_caption_test = len(caption_ids)\n",
    "    num_batches = n_caption_test // cfg.BATCH_SIZE\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        for per_caption in range(cfg.TEXT.CAPTIONS_PER_IMAGE):\n",
    "            test_cap = caption_ids[i * cfg.BATCH_SIZE: (i + 1) * cfg.BATCH_SIZE, per_caption]\n",
    "            test_directory = test_dataset.filenames[i * cfg.BATCH_SIZE: (i + 1) * cfg.BATCH_SIZE]\n",
    "\n",
    "            z = np.random.normal(loc=0.0, scale=1.0, size=(cfg.BATCH_SIZE, cfg.GAN.Z_DIM)).astype(np.float32)\n",
    "            gen = sess.run(generator.outputs, feed_dict={t_real_caption: test_cap, t_z: z})\n",
    "            \n",
    "            for j in range(cfg.BATCH_SIZE):\n",
    "                if not os.path.exists(os.path.join(cfg.TEST.GENERATED_TEST_IMAGES, test_directory[j].split('/')[0])):\n",
    "                    os.mkdir(os.path.join(cfg.TEST.GENERATED_TEST_IMAGES, test_directory[j].split('/')[0]))\n",
    "\n",
    "                scipy.misc.imsave(os.path.join(cfg.TEST.GENERATED_TEST_IMAGES, test_directory[j] + '_{}.png'.format(per_caption)), gen[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_r_precision_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_inception_score_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Measure Inception score and R-precision of given test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After set the config file as 'eval_birds.yml' and run the 'generate_inception_score_data()' and 'generate_r_precision_data()', the synthesized images based on given captions and set of image and caption features should be saved inside a 'evaluation' folder, specifically in 'evaluation/generated_images/..' and as 'evaluation/r_precision.npz' respectively.\n",
    "\n",
    "**Then, go to the 'evaluation' folder and run each 'inception_score.ipynb' and 'r_precision.ipynb' file in order to measure inception score and r-precision score.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anna_36]",
   "language": "python",
   "name": "conda-env-anna_36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
